{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qe6yTh55trpQ"
   },
   "source": [
    "# Assignment 1, Task 3: Multilayer Perceptron (MLP)\n",
    "This is the third part of the assignment. You will get to implement MLP using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Vs2WYIFtrpS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gYnTjputrpV"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "I31uJ6KltrpW",
    "outputId": "1a677958-43ca-422c-ec66-38e8cdff4283",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, test = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_test_raw, y_test = test\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_test = X_test_raw.reshape((X_test_raw.shape[0], X_test_raw.shape[1]**2))\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 58000 samples from original training set of 60,000\n",
    "# Validation data: 2000 samples from original training set: 58,000~60,000\n",
    "# Test data: 10000 samples from original test set\n",
    "# Development data (for gradient check): 100 from the training set\n",
    "\n",
    "num_training = 58000\n",
    "num_validation = 2000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xa_w8ZHbtrqY"
   },
   "source": [
    "## Part 1: Tensorflow MLP (10%)\n",
    "In this part, you will use tensorflow modules to implement an MLP. We provide a demo of a two-layer net, the style of is referenced from https://www.tensorflow.org/guide/keras and https://www.tensorflow.org/guide/eager. \n",
    "\n",
    "You need to implement a MLP with 4 layers in a similar style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Two-layer MLP in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "-0xQOzpdtrqZ",
    "outputId": "212defc2-2cd6-406f-affe-9c6c7367e87f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Demo: Two-layer net in tensorflow (eager execution mode)\n",
    "hidden_dim = 300\n",
    "reg_tf = tf.constant(0.01)\n",
    "\n",
    "# Define a tf.keras.Model class\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.W1 = tf.Variable(1e-2*np.random.rand(X_train.shape[1], hidden_dim).astype('float32'))\n",
    "        self.b1 = tf.Variable(np.zeros((hidden_dim,)).astype('float32'))\n",
    "        self.W2 = tf.Variable(1e-2*np.random.rand(hidden_dim, 20).astype('float32'))\n",
    "        self.b2 = tf.Variable(np.zeros((20,)).astype('float32'))\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        h1 = tf.nn.relu(tf.matmul(inputs, self.W1) + self.b1)\n",
    "        out = tf.matmul(h1, self.W2) + self.b2\n",
    "        return out\n",
    "\n",
    "# Define and calculate loss function (Note that in eager execution, loss must be in a function)\n",
    "def loss(model, inputs, targets, reg = tf.constant(0.01)):\n",
    "    out = model(inputs)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits= out, labels=tf.one_hot(targets,20))\n",
    "    L2_loss = tf.nn.l2_loss(model.W1) + tf.nn.l2_loss(model.W2)\n",
    "    return tf.reduce_mean(cross_entropy) + reg * L2_loss\n",
    "\n",
    "# Calculate gradients for all variables using tf.GradientTape\n",
    "def grad(model, inputs, targets, reg = tf.constant(0.01)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, reg=reg)\n",
    "    return tape.gradient(loss_value, [model.W1, model.b1, model.W2, model.b2])\n",
    "\n",
    "# Calculate classification accuracy\n",
    "def eval_acc(model, inputs, targets):\n",
    "    correct_prediction = tf.equal(targets, tf.cast(tf.argmax(model(inputs),1), tf.uint8))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "num_train = 58000\n",
    "batch_size = 500\n",
    "num_batch = num_train//batch_size\n",
    "num_epochs = 15\n",
    "model = Model()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    for i in range(num_batch):\n",
    "        batch_xs, batch_ys = X_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "        x_tf = tf.Variable(batch_xs, dtype = tf.float32)\n",
    "        y_tf = tf.Variable(batch_ys, dtype = tf.uint8)\n",
    "        \n",
    "        grads = grad(model, x_tf, y_tf, reg_tf)\n",
    "        # Optimization based on calculated gradients \n",
    "        optimizer.apply_gradients(zip(grads, [model.W1, model.b1, model.W2, model.b2]))\n",
    "\n",
    "    x_tf = tf.Variable(X_val, dtype = tf.float32)\n",
    "    y_tf = tf.Variable(y_val, dtype = tf.uint8)\n",
    "    accuracy = eval_acc(model, x_tf, y_tf)\n",
    "    val_acc = accuracy.numpy()\n",
    "    print('epoch {}: valid acc = {}'.format(e+1, val_acc))\n",
    "\n",
    "x_tf = tf.Variable(X_test, dtype = tf.float32)\n",
    "y_tf = tf.Variable(y_test, dtype = tf.uint8)\n",
    "accuracy = eval_acc(model, x_tf, y_tf)\n",
    "test_acc = accuracy.numpy()\n",
    "print('test acc = {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Deeper Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YmSduBmytrqb"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Create your MLP in tensorflow. Since you are going to create a deeper neural network, it is recommended to use \"list\" to store your network parameters (weights and bias). Consider using a loop to create your MLP network.\n",
    "\n",
    "<span style=\"color:red\"><strong>HINT</strong></span>: Copy the code above and make necessary changes in model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create you MLP using TensorFlow functions.\n",
    "\n",
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Introduction to TensorFlow.keras (10%)\n",
    "\n",
    "As you can see, when the network structure becomes larger, it is hard to handle variables from every layer. Here we introduce the `tf.keras` tool to build the network in a much simpler way. You may want to use it in your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Follow this official example: https://www.tensorflow.org/datasets/keras_example#step_2_create_and_train_the_model to build an MLP and train it. \n",
    "\n",
    "*You should keep the same optimizer (SGD) and loss function (cross entropy) as in the previous task.*\n",
    "\n",
    "**Note:** Since we want to use our own dataset, we will not use the `tfds.load` method to load the data this time. \n",
    "\n",
    "You need to check the usage of [`model.fit()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) and feed the model with our own data.\n",
    "\n",
    "**Tips:**\n",
    "* Softmax is also registered as a layer operation in tf.keras.\n",
    "* You can use `model.summary()` to visualize the model after you build it.\n",
    "* Use `verbose=2` in `model.fit()` to get similar training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# TODO: Build the model with tf.keras.models.Sequential                    #\n",
    "############################################################################\n",
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# TODO: Compile the model, set optimizer and loss                          #\n",
    "############################################################################\n",
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# TODO: Train the model with our own dataset                               #\n",
    "############################################################################\n",
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: t-SNE (5%)\n",
    "\n",
    "**t-SNE** is is a machine learning algorithm for nonlinear dimensionality reduction developed by *Geoffrey Hinton* and *Laurens van der Maaten*. It can be used as a good way of visualizing high-dimensional data in 2D.\n",
    "\n",
    "Here we show its application for CIFAR100. Later it will be re-used in a CNN network. Experimenting with t-SNE can be fun. One thing to try is to visualize the output of each layer of MLP to observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_funcs import train, test\n",
    "from utils.classifiers.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train_data, test_data = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train_data\n",
    "X_test_raw, y_test = test_data\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_test = X_test_raw.reshape((X_test_raw.shape[0], X_test_raw.shape[1]**2))\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 58000 samples from original training set of 60,000\n",
    "# Validation data: 2000 samples from original training set: 58,000~60,000\n",
    "# Test data: 10000 samples from original test set\n",
    "# Development data (for gradient check): 100 from the training set\n",
    "num_training = 58000\n",
    "num_validation = 2000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE of Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**NOTE**</font>: You may have to restart the kernel before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "random_select = np.random.choice(10000, 500, replace=False)\n",
    "X = X_test_raw[random_select,:,:].reshape(500,X_train.shape[1]).astype('float')/255.0\n",
    "\n",
    "tic = time.time()\n",
    "Y = TSNE(n_components=2, perplexity=25.0).fit_transform(X)\n",
    "print(\"it takes {} seconds\".format(time.time()-tic))\n",
    "\n",
    "## Visualize t-SNE of original data\n",
    "labels = y_test[random_select]\n",
    "colors = np.random.rand(20,3)\n",
    "color_labels = [colors[int(i)] for i in labels.tolist()]\n",
    "plt.scatter(Y[:,0], Y[:,1], 20, color_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE of Data After Two Hidden Layers\n",
    "\n",
    "Visualize the t-SNE of data after going through MLP. In the visualization result, you should find that the t-SNE of the original data points are rather jumbled up. However, visualization of data after the trained two-layer networks are clearly separated into multiple clusters in a 2D panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define MLP model\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100], num_classes=20, reg=0.1, weight_scale=1e-3)\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 200\n",
    "lr = 1e-3\n",
    "verbose = False\n",
    "train_acc_hist, val_acc_hist = train(model, X_train, y_train, X_val, y_val, \n",
    "                  num_epoch=num_epoch, batch_size=batch_size, learning_rate=lr, verbose=verbose)\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Visualize data that is passed through the MLP model defined above using t-SNE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run tSNE\n",
    "tic = time.time()\n",
    "\n",
    "#############################################################################\n",
    "#                          START OF YOUR CODE                               #\n",
    "# Hint: Pass data through affine and dense layers (model.layers) and then   # \n",
    "# apply softmax to obtain output of the MLP model.                          #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n",
    "print(\"it takes {} seconds\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize t-SNE 2D representation of data after two hidden layers\n",
    "#############################################################################\n",
    "#                          START OF YOUR CODE                               #\n",
    "# Hint: See t-SNE visualization of original data.                           #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Try tuning the parameters of t-SNE. Do visualization of the new t-SNE of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the parameter, show the results.\n",
    "# Run t-SNE\n",
    "tic = time.time()\n",
    "\n",
    "#############################################################################\n",
    "#                          START OF YOUR CODE                               #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n",
    "print(\"it takes {} seconds\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize t-SNE 2D representation of data after two hidden layers\n",
    "#############################################################################\n",
    "#                          START OF YOUR CODE                               #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task2-mlp_eager.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
