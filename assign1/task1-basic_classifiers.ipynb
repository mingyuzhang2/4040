{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qe6yTh55trpQ"
   },
   "source": [
    "# Assignment 1, Task 1: Basic Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7OSce2LHK_B"
   },
   "source": [
    "In this task, you are going to implement two classifers and apply them to the **Fashion-MNIST** dataset: \n",
    "\n",
    "(1) Logistic Regression (LR) classifier\n",
    "\n",
    "(2) Softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mLE5oHAkHK_C"
   },
   "outputs": [],
   "source": [
    "# Import modules, make sure you have installed all required packages before you start.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Module auto reloading. (Ref: https://ipython.org/ipython-doc/3/config/extensions/autoreload.html)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMZSC2GLHK_F"
   },
   "source": [
    "## Load Fashion-MNIST Dataset\n",
    "\n",
    "[**Fashion-MNIST**](https://github.com/zalandoresearch/fashion-mnist) is a widely used dataset mainly used for benchmarking the very basic machine learning models. Images are drawn from Zalando's clothing articles and the dataset consists of a training set with 60,000 examples and a test set with 10,000 examples. Each example is a $28 \\times 28$ pixel grayscale image with an associated label from 10 classes. We will use this to create our training set, validation set, and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the raw Fashion-MNIST data to create a 10-class dataset and manually define a label map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7274,
     "status": "ok",
     "timestamp": 1559884655914,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "bV0hpeEqHK_G",
    "outputId": "c4b54f60-33c3-4d00-c783-316c1de30020",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, test = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_test_raw, y_test = test\n",
    "\n",
    "# The integer labels in y_train and y_test correspond to the index of this label map\n",
    "label_map = [\n",
    "    't-shirt/top', 'trouser', 'pullover', 'dress', 'coat', \n",
    "    'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'\n",
    "]\n",
    "\n",
    "# Here we vectorize the data (rearranged the shape of images) for you. \n",
    "# That is, we flatten 28×28 2D images into 784 1D arrays.\n",
    "# The reason we do this is because we cannot input 2D image representations into our model. \n",
    "# This is a common practice (flattening images before feeding them into the ML models). \n",
    "# Note that this isn't the usual practice for Convolutional Neural Networks (CNN). \n",
    "# We will see how we manage the data with CNNs in later assignments.\n",
    "\n",
    "# Check the results\n",
    "print('Raw training data shape: ', X_train_raw.shape)\n",
    "print('Raw test data shape: ', X_test_raw.shape)\n",
    "\n",
    "# API Reference: https://numpy.org/doc/stable/reference/generated/numpy.reshape.html\n",
    "# In practice, you would ALWAYS want to consult the documentations for API usage.\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_test = X_test_raw.reshape((X_test_raw.shape[0], X_test_raw.shape[1]**2))\n",
    "\n",
    "num_classes = max(y_train) + 1\n",
    "\n",
    "print('Number of classes: ', num_classes)\n",
    "print('Vectorized training data shape: ', X_train.shape)\n",
    "print('Vectorized test data shape: ', X_test.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8435,
     "status": "ok",
     "timestamp": 1559884658097,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "6n4v89BcHK_L",
    "outputId": "9141d16e-b67a-4d60-a7ed-0eb350bc4748",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's randomly select 30 images from the training set for visualization.\n",
    "ids = np.random.randint(X_train_raw.shape[0], size=30).reshape((6, 5))\n",
    "\n",
    "# Visualize Fashion-MNIST data.\n",
    "fig, axes1 = plt.subplots(6, 5, figsize=(8, 8))\n",
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "        axes1[i][j].set_axis_off()\n",
    "        axes1[i][j].imshow(X_train_raw[ids[i][j]], cmap='gray')\n",
    "        axes1[i][j].set_title(label_map[y_train[ids[i][j]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1559884658978,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "6YoejUt3HK_O",
    "outputId": "8f961d92-4960-4fb2-d496-5e0521996c7a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data organization:\n",
    "#    Training data: 49,000 samples from the original train set: indices 1~49,000\n",
    "#    Validation data: 1,000 samples from the original train set: indices 49,000~50,000\n",
    "#    Test data: 1,000 samples from the original test set: indices 1~1,000\n",
    "#    Development data (for gradient check): 100 random samples from the training set: indices 1~49,000\n",
    "#    Development data (binary) (only for gradient check in Part 1): 100 random samples from the binary training set\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 100\n",
    "num_dev_binary = 100\n",
    "\n",
    "# For numpy slicing operations, refer to\n",
    "# https://numpy.org/doc/stable/user/basics.indexing.html\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "X_test = X_test[:num_test, :]\n",
    "y_test = y_test[:num_test]\n",
    "\n",
    "ids_dev = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[ids_dev]\n",
    "y_dev = y_train[ids_dev]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we subsample the whole (10-class) dataset to obtain the binary (2-class) dataset for our LR classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subsample 10-class training set to 2-class training set (class label 5 and 8)\n",
    "X_train_binary = X_train[(y_train == 5) | (y_train == 8)]\n",
    "y_train_binary = y_train[(y_train == 5) | (y_train == 8)]\n",
    "\n",
    "X_val_binary = X_val[(y_val == 5) | (y_val == 8)]\n",
    "y_val_binary = y_val[(y_val == 5) | (y_val == 8)]\n",
    "\n",
    "ids_dev_binary = np.random.choice(X_train_binary.shape[0], num_dev_binary, replace=False)\n",
    "X_dev_binary = X_train_binary[ids_dev_binary]\n",
    "y_dev_binary = y_train_binary[ids_dev_binary]\n",
    "\n",
    "print('Train data (binary) shape: ', X_train_binary.shape)\n",
    "print('Train labels (binary) shape: ', y_train_binary.shape)\n",
    "print('Validation data (binary) shape: ', X_val_binary.shape)\n",
    "print('Validation labels (binary) shape: ', y_val_binary.shape)\n",
    "print('Development data (binary) shape:', X_dev_binary.shape)\n",
    "print('Development labels (binary) shape', y_dev_binary.shape)\n",
    "\n",
    "print('Classes in training set:', np.unique(y_train_binary))\n",
    "print('Classes in validation set:', np.unique(y_val_binary))\n",
    "print('Classes in development set:', np.unique(y_dev_binary))\n",
    "\n",
    "print('Class distribution in training set:')\n",
    "print(np.bincount(y_train_binary, minlength=10))\n",
    "print('Class distribution in validation set:')\n",
    "print(np.bincount(y_val_binary, minlength=10))\n",
    "print('Class distribution in development set:')\n",
    "print(np.bincount(y_dev_binary, minlength=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "As a preparation, let's normalize the input data:\n",
    "* Training data is 2D with shape `[num_training=49,000, dim=784]`.\n",
    "* Obtain the **mean** ($\\mu$) and **standard deviation** ($\\sigma$) of all samples.\n",
    "* For every training sample, subtract the mean and divide by the standard deviation.\n",
    "\n",
    "The ***normalized*** training data $X$ now has a standard form of $\\mu (X) = 0$ and $\\sigma (X) = 1$. We will revisit input normalization in detail in later assignments. For short, the intuition here is to eliminate the effect of input range on the model.\n",
    "\n",
    "<font color=\"red\"><strong>Note</strong></font>: We always use the statistics from the training to normalize validation & test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2569,
     "status": "ok",
     "timestamp": 1559884662531,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "XWboKTBmHK_S",
    "outputId": "a81860fe-983b-44c7-8382-0b68b396815f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For the original dataset\n",
    "# Please, for yourself, search and understand the usage of common numpy APIs.\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train.astype(np.float32) - mean) / std\n",
    "X_val = (X_val.astype(np.float32) - mean) / std\n",
    "X_test = (X_test.astype(np.float32) - mean) / std\n",
    "X_dev = (X_dev.astype(np.float32) - mean) / std\n",
    "\n",
    "# For the binary dataset\n",
    "mean_bin = np.mean(X_train_binary, axis=0)\n",
    "std_bin = np.std(X_train_binary, axis=0)\n",
    "\n",
    "X_train_binary = (X_train_binary.astype(np.float32) - mean_bin) / std_bin\n",
    "X_val_binary = (X_val_binary.astype(np.float32) - mean_bin) / std_bin\n",
    "X_dev_binary = (X_dev_binary.astype(np.float32) - mean_bin) / std_bin\n",
    "\n",
    "print('Train data mean: ', np.mean(X_train))\n",
    "print('Train data std: ', np.std(X_train))\n",
    "print('Validation data mean: ', np.mean(X_val))\n",
    "print('Validation data std: ', np.std(X_val))\n",
    "print('Test data mean: ', np.mean(X_test))\n",
    "print('Test data std: ', np.std(X_test))\n",
    "print('Train data (binary) mean: ', np.mean(X_train_binary))\n",
    "print('Train data (binary) std: ', np.std(X_train_binary))\n",
    "print('Validation data (binary) mean: ', np.mean(X_val_binary))\n",
    "print('Validation data (binary) std: ', np.std(X_val_binary))\n",
    "print('Development data (binary) mean: ', np.mean(X_dev_binary))\n",
    "print('Development data (binary) std: ', np.std(X_dev_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are going to get familiar with one of the very basic modeling methodologies of classification problems. \n",
    "\n",
    "<font color=\"red\"><strong>Note</strong></font>: Without further specifications, all vectors mentioned are considered ***column*** vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s consider a training dataset of $N$ images. Each of the images $x_i$ has an associated label $y_i \\in \\{1, \\dots, K\\}$, where $i = \\{1, \\dots, N\\}$ and $K$ represents the number of distinct classes.\n",
    "\n",
    "Assume that all images have shape `[H(eight), W(idth), C(hannels)]`, a ***flattened*** image can be denoted as\n",
    "\n",
    "$$\n",
    "x_i \\in R^D \\quad \\text{where} \\quad D = H \\times W \\times C\n",
    "$$\n",
    "\n",
    "And we construct the dataset $(X, y)$ as\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} x_1^T \\\\ \\vdots \\\\ x_N^T \\end{bmatrix} \\in R^{N \\times D}, \\quad\n",
    "y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix} \\in R^N\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Fashion-MNIST dataset as an example, we have training data `X_train` with shape `[49000,784]` and the corresponding labels `y_train` with shape `[49000,]`. Here the number of input images $N = 49000$, and each image has a flattened shape of $D = 28 \\times 28 \\times 1 = 784$. The total number of classes is $K = 10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Logistic Regression Classifier (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are going to implement a Logistic Regression (LR) classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Problem Definition\n",
    "\n",
    "Logistic Regression classifier can solve a binary classification problem that has only two classes. Therefore, it is reasonable for us to model only one of the two classes. Specifically, we develop a model that try to assign the samples to one class, and those sample that cannot be assigned should be considered as the other class [3].\n",
    "\n",
    "To put it mathematically, consider some sample $x_i \\in X$ with the corresponding binary label $y_i = \\{0, 1\\}$. We study the ***likelihood*** of this sample belonging to class \"$1$\" under model parameter $w$, i.e. $P(y_i = 1 | x_i, w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Formulation\n",
    "\n",
    "Consider a ***linear*** score function $f: R^D \\to R$ that takes in a sample $x_i$ and computes a class score (also called a \"***logit***\")\n",
    "\n",
    "$$f(x_i; w, b) := w^T x_i + b$$\n",
    "\n",
    "where $w \\in R^D$ contains the weights and $b \\in R$ is the bias.\n",
    "\n",
    "The LR model is defined to be a mapping from a sample $x_i \\in R^D$ to the probability\n",
    "\n",
    "$$P(y_i = 1 | x_i, w) := \\sigma (f_i) = \\frac{1}{1 + e^{-f_i}}$$\n",
    "\n",
    "where $f_i = f(x_i; w, b)$ and $\\sigma: R \\to (0, 1)$ is the [`sigmoid`](https://en.wikipedia.org/wiki/Sigmoid_function) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make life easier, we use the ***bias trick*** to represent the two parameters ($w, b$) as one by the two following steps:\n",
    "1. Extend the vector $x_i$ with one additional constant $1$\n",
    "$$x_i \\gets [x_i; 1] \\in R^{D + 1}$$\n",
    "2. Concatenate the original weights $w$ and biases $b$ together to form a new weight vector\n",
    "$$w \\gets [w; b] \\in R^{D + 1}$$\n",
    "\n",
    "Without changing the notations, the score function simplifies to\n",
    "\n",
    "$$f(x_i; w) = w^T x_i$$\n",
    "\n",
    "where we have $w, x_i \\in R^{D + 1}$. Indeed, this is a ***homogeneous*** representation of the linear system.\n",
    "\n",
    "Therefore, the likelihood predicted by the LR model is defined as\n",
    "\n",
    "$$P(y_i = 1 | x_i, w) = \\sigma(f_i) = \\frac{1}{1 + e^{-w^T x_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the mappings defined above, a sample $x_i$ can be classified by setting a probability threashold $T = 0.5$. Pariticularly, we predict a sample $x_i$ to be of class \"$1$\" if \n",
    "\n",
    "$$P(y_i = 1 | x_i, w) > T$$\n",
    "\n",
    "It is easy to show that this is equivalent to $f_i > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Optimization\n",
    "\n",
    "Our model can now make predictions on input samples in $R^{D + 1}$. We can thus evaluate the quality of the predictions.\n",
    "\n",
    "For some $x_i$, the likelihood of our model making ***correct*** predictions (same as the ground truth) can be given by\n",
    "\n",
    "$$\n",
    "P(y_i | x_i, w) = \\begin{cases} \n",
    "p_i & \\text{if } y_i = 1 \\\\ \n",
    "1 - p_i & \\text{otherwise } (\\text{i.e. } y_i = 0)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $p_i = P(y_i = 1 | x_i, w)$.\n",
    "\n",
    "It is not hard to show that this equation can then be re-written in a more compact form [3] as\n",
    "\n",
    "$$P(y_i | x_i, w) = p_i^{y_i} (1 - p_i)^{1 - y_i}$$\n",
    "\n",
    "You can simply verify this by plugging in $y_i = 0$ and $y_i = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a good model, we can estimate the parameter $w$ such that our model can achieve the maximum likelihood of making correct predictions over all samples. This is called the ***Maximum Likelihood Estimation (MLE)***.\n",
    "\n",
    "Assume all $x_i$ are [i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables), we seek to maximize the joint likelihood, i.e.\n",
    "\n",
    "$$\\max_w P(y | X, w)$$\n",
    "\n",
    "where $P(y | X, w) = \\Pi_{i=1}^N P(y_i|x_i,w)$.\n",
    "\n",
    "Maximizing this joint likelihood directly can be challenging, so we take the natural logarithm of the likelihood function, converting the product into a sum. Because the logarithm is a monotonically increasing function, maximizing the likelihood is equivalent to maximizing the ***log-likelihood***:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l(w) &= \\ln P(y | X, w) \\\\\n",
    "     &= \\ln \\left(\\prod_{i=1}^N P(y_i | x_i, w)\\right) \\\\\n",
    "     &= \\sum_{i=1}^N y_i \\ln p_i + (1 - y_i) \\ln (1 - p_i)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, it is common to formulate problems as minimization rather than maximization problems. Therefore, we convert the maximization of the log-likelihood into a minimization problem by considering the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "\\min_w -l(w)\n",
    "$$\n",
    "\n",
    "This negative log-likelihood is also known as the **binary cross-entropy loss**, which is widely used for binary classification tasks.\n",
    "\n",
    "To prevent overfitting and improve generalization, we introduce a regularization term. Specifically, we use $L_2$ regularization (also known as weight decay), which penalizes large weights by adding a term proportional to the square of the $L_2$ norm of the weights. The regularization term is $\\frac{\\lambda}{2} \\|w\\|^2$, where $\\lambda$ is the regularization coefficient. Incorporating this into the objective function, we have:\n",
    "\n",
    "$$\n",
    "\\min_w \\left[ -\\frac{1}{N} l(w) + \\frac{\\lambda}{2} \\|w\\|^2 \\right]\n",
    "$$\n",
    "\n",
    "Finally, combining the binary cross-entropy loss with the $L_2$ regularization, we derive the complete objective function $L(w; X, y)$ that we aim to minimize:\n",
    "\n",
    "$$\n",
    "L(w; X, y) = -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\ln p_i + (1 - y_i) \\ln (1 - p_i)\\right] + \\frac{\\lambda}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "In this formula, the first term represents the average binary cross-entropy loss, while the second term represents the $L_2$ regularization. The goal is to minimize this objective function to find the optimal parameter $w$ that balances fitting the data well and keeping the model complexity under control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no analytic solution to the problem in general. Nonetheless, we can find the optimum by ***Gradient Descent***.\n",
    "\n",
    "One can calculate\n",
    "\n",
    "$$\\nabla_w L = - \\frac{1}{N} \\sum_{i=1}^N (y_i - p_i) x_i + \\lambda w$$\n",
    "\n",
    "For a clearer notation,\n",
    "\n",
    "$$\n",
    "\\nabla_w L = - \\frac{1}{N} X^T (y - p) + \\lambda w\n",
    "\\quad \\text{where} \\quad\n",
    "p := \\begin{bmatrix} p_1 \\\\ \\vdots \\\\ p_N \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Recall that $X \\in R^{N \\times (D + 1)}$, and the gradient $\\nabla_w L \\in R^{D + 1}$ must be of the same shape with the parameter $w$.\n",
    "\n",
    "<font color=\"red\"><strong>Note</strong></font>: This solution is valid due to the ***convexity*** of our formulation in nature. It is worth keeping in mind that gradient methods DO NOT guarantee the optimal solution for non-convex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to explore these useful links:\n",
    "\n",
    "[1] https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "\n",
    "[2] https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac\n",
    "\n",
    "[3] Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_sQKxgGHK_V"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Based on the derivations given above, complete the code in `./utils/classifiers/logistic_regression.py`. You have to implement the classifier in two ways: \n",
    "\n",
    "* Naive method using for-loop: Loop over and compute each element\n",
    "* Vectorized method: Use the NumPy vector/matrix operations\n",
    "\n",
    "We provide the verification code for you to check if your implementation is correct. \n",
    "\n",
    "<font color=\"red\"><strong>Hint</strong></font>:\n",
    "* Do not forget the $L_2$ regularization term in the loss function and the gradients.\n",
    "* Be aware of the data types. It is a good habit to avoid any numerical computations between different data types.\n",
    "\n",
    "A Tensorflow implementation of LR is demonstrated in the verification code below. This step will familiarize you with TensorFlow functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>Note</strong></font>: Please do not change the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1559884668395,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "v0DB0KWoHK_Y",
    "outputId": "eb47b0b6-6b34-4625-9a24-03dc219914fd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verification code for checking the correctness of the implementation of logistic_regression\n",
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT\n",
    "\n",
    "from utils.classifiers.logistic_regression import logistic_regression_loss_naive\n",
    "from utils.classifiers.logistic_regression import logistic_regression_loss_vectorized\n",
    "\n",
    "# Generate initial weight vector\n",
    "w = np.random.randn(X_train.shape[1]) * 0.0001\n",
    "reg = 0.000005\n",
    "\n",
    "# Naive numpy implementation of Logistic Regression\n",
    "loss_naive, grad_naive = logistic_regression_loss_naive(w, X_dev_binary, y_dev_binary, reg)\n",
    "\n",
    "# Vectorized numpy implementation of Logistic Regression\n",
    "loss_vec, grad_vec = logistic_regression_loss_vectorized(w, X_dev_binary, y_dev_binary, reg)\n",
    "\n",
    "# True value computed by tf\n",
    "# Here we specify float64 because this is the default float type in numpy\n",
    "# A usual float32 may result in inconsistancy due to floating point error\n",
    "w_tf = tf.Variable(w, dtype=tf.float64)\n",
    "with tf.GradientTape() as tape:\n",
    "     tape.watch(w_tf)\n",
    "     # loss calculation:        use a tf.nn loss function\n",
    "     loss_true = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          tf.Variable(y_dev_binary, dtype=tf.float64), \n",
    "          # do w@x\n",
    "          tf.linalg.matvec(tf.Variable(X_dev_binary, dtype=tf.float64), w_tf)\n",
    "          # regularization\n",
    "     )) + reg * tf.nn.l2_loss(w_tf)\n",
    "     # gradient calculation\n",
    "     grad_true = tape.gradient(loss_true, w_tf)\n",
    "\n",
    "## Check the correctness\n",
    "print('naive numpy loss: {}.'.format(loss_naive))\n",
    "print('vectorized numpy loss: {}.'.format(loss_vec))\n",
    "print('true loss: {}'.format(loss_true))\n",
    "print('*'*100)\n",
    "print('Relative naive gradient error is {}'.format(np.linalg.norm(grad_naive - grad_true)))\n",
    "print('Relative vectorized gradient error is {}'.format(np.linalg.norm(grad_vec - grad_true)))\n",
    "print('*'*100)\n",
    "print('Is naive loss correct? {}'.format(np.allclose(loss_naive, loss_true)))\n",
    "print('Is naive gradient correct? {}'.format(np.allclose(grad_naive, grad_true)))\n",
    "print('Is vectorized loss correct? {}'.format(np.allclose(loss_vec, loss_true)))\n",
    "print('Is vectorized gradient correct? {}'.format(np.allclose(grad_vec, grad_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Softmax Classifier (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax classifier generalizes the Logistic Regression classifier to multiple classes.\n",
    "\n",
    "In the Softmax classifier, the score function is a mapping from a sample $x_i \\in R^{D + 1}$ to $K$ different class scores, i.e.\n",
    "\n",
    "$$f_{i,k} = w_k^T x_i \\quad \\forall k \\in \\{1, \\dots, K\\}$$\n",
    "\n",
    "Or equivalently, we have $f: R^{D + 1} \\to R^K$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x_i; W) \n",
    "&:= \\begin{bmatrix} f_{i,1} \\\\ \\vdots \\\\ f_{i,K} \\end{bmatrix}\n",
    "= \\begin{bmatrix} w_1^T x_i \\\\ \\vdots \\\\ w_K^T x_i \\end{bmatrix}\n",
    "= W^T x_i \\\\\n",
    "&\\text{where} \\quad\n",
    "W = [w_1, \\dots, w_K] \\in R^{(D + 1) \\times K}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice that this is also a homogeneous representation where we've already applied the bias trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain the predicted probabilities over $K$ classes, here we introduce the [`softmax`](https://en.wikipedia.org/wiki/Softmax_function) function $\\sigma: R^K \\to (0, 1)^K$\n",
    "\n",
    "$$\\sigma (s) := \\frac{e^s}{\\sum_{k=1}^K e^{s_k}}$$\n",
    "\n",
    "Indeed, the softmax function results in a probability distribution, i.e. $\\sum_k \\sigma (s)_k = 1$.\n",
    "\n",
    "And the predicted distribution for sample $x_i$ can be defined as\n",
    "\n",
    "$$P(y_i | x_i, W) := \\sigma (f_i) = \\frac{e^{f_i}}{\\sum_{k=1}^K e^{f_{i,k}}}$$\n",
    "\n",
    "where $f_i = f(x_i; W)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we have to measure the quality of our predictions.\n",
    "\n",
    "For some (discrete) distributions $q, r \\in R^K$, consider\n",
    "\n",
    "$$H(q, r) := -\\mathbb{E}_q [\\ln r] = -\\sum_{k = 1}^K q_k \\ln r_k$$\n",
    "\n",
    "This real-valued function is called the [***cross-entropy***](https://en.wikipedia.org/wiki/Cross-entropy), which can be regarded as a ***dissimilarity metric*** between $p$ and $q$.\n",
    "\n",
    "For some sample $x_i$, the target distribution derived from the corresponding label $y_i \\in \\{1, \\dots, K\\}$ can be constructed using [***one-hot encoding***](https://en.wikipedia.org/wiki/One-hot), which reads\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{matrix} \n",
    "g_i := &[ &0, &\\dots, &0, &\\underline{\\underline{1}}, &0, &\\dots, &0 &] \\in R^K \\\\\n",
    "& &1\\text{-st} &\\dots & &y_i\\text{-th} &\\dots & &K\\text{-th} &\n",
    "\\end{matrix} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The ideal distribution only takes $1$ at the $y_i$-th position and has $0$ everywhere else. The dissimilarity between prediction $p_i := P(y_i | x_i, W)$ and target $g_i$ is then $H(g_i, p_i) = -g_i^T \\ln p_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7u9-MqILHK_b"
   },
   "source": [
    "Intuitively, we optimize the model parameter $W$ by minimizing the dissimilarity between predictions and targets over all samples, i.e.\n",
    "\n",
    "$$\n",
    "\\min_W L(W; X, y) \\quad \\text{s.t.} \\quad\n",
    "L(W; X, y) = \\frac{1}{N} \\sum_{i = 1}^N H(g_i, p_i) + \\frac{\\lambda}{2} \\|W\\|_F^2\n",
    "$$\n",
    "\n",
    "This is indeed one of the most widely used classification loss known as the ***categorical cross-entroy***.\n",
    "\n",
    "Here, $\\|W\\|_F = \\sqrt{\\sum_{k,i} |w_{k,i}|^2}$ is the [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm). It is easy to see that\n",
    "\n",
    "$$\\|W\\|_F^2 = \\sum_k \\|w_k\\|^2$$\n",
    "\n",
    "which corresponds to the sum of the $L_2$ norms of weights $w_k$ of every class from $1$ to $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the gradient follows  \n",
    "\n",
    "$$\n",
    "\\nabla_{w_k} L= \\frac{1}{N} \\sum_i (p_{i,k} - g_{i,k}) x_i + \\lambda w_k\n",
    "\\quad \\forall k \\in \\{1, \\dots, K\\}\n",
    "$$\n",
    "\n",
    "Or equivalently, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_W L &= \\frac{1}{N} X^T (P - G) + \\lambda W \\\\\n",
    "&\\text{where} \\quad\n",
    "G = \\begin{bmatrix} g_1^T \\\\ \\vdots \\\\ g_N^T \\end{bmatrix} \\in R^{N \\times K},\n",
    "P = \\begin{bmatrix} p_1^T \\\\ \\vdots \\\\ p_N^T \\end{bmatrix} \\in R^{N \\times K}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "And obviously, $\\nabla_W L \\in R^{(D + 1) \\times K}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: **Numerical Stability**. When you are writing code for computing the `softmax` function in practice, the intermediate terms $e^s$ and $\\sum_k e^{s_k}$ may be very large due to the exponentials. Division with large numbers can be numerically unstable, so it is important to use the normalization trick. \n",
    "\n",
    "Notice that if we multiply both the top and the bottom of the fraction by constant $C$ and push $C$ inside the exponent, we get the following equivalent expression: \n",
    "\n",
    "$$\n",
    "\\sigma (s)\n",
    "= \\frac{e^s}{\\sum_k e^{s_k}}\n",
    "= \\frac{C e^s}{C \\sum_k e^{s_k}}\n",
    "= \\frac{e^{s + \\ln C}}{\\sum_k e^{s_k + \\ln C}}\n",
    "$$\n",
    "\n",
    "A common choice for $C$ is to set $\\ln C= -\\max_i s_i$ to be the maximum among all components of $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7u9-MqILHK_b"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Based on the derivations above, complete the code in **./utils/classifiers/softmax.py**. You have to implement the classifier in two ways: \n",
    "\n",
    "* Naive method using for-loop: Loop over and compute each element\n",
    "* Vectorized method: Use the NumPy vector/matrix operations\n",
    "\n",
    "We provide the verification code for you to check if your implementation is correct. \n",
    "\n",
    "<font color=\"red\"><strong>Hint:</strong></font>\n",
    "* Do not forget the $L_2$ regularization term in the loss function and the gradients.\n",
    "* Be aware of the data types. It is a good habit to avoid any numerical computations between different data types.\n",
    "\n",
    "A Tensorflow implementation of Softmax classifier is demonstrated in the verification code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1559884674707,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "DH7HdbEAHK_b",
    "outputId": "a6d3eb81-af5a-427e-f25e-4c811e36d847",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verification code for checking the correctness of the implementation of softmax implementations\n",
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.classifiers.softmax import softmax_loss_naive\n",
    "from utils.classifiers.softmax import softmax_loss_vectorized\n",
    "\n",
    "## generate a random weight matrix of small numbers\n",
    "W = np.random.randn(X_train.shape[1], num_classes) * 0.0001\n",
    "reg = 0.000005\n",
    "\n",
    "## naive softmax in numpy\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive numpy loss: {}, takes {} seconds.'.format(loss_naive, toc - tic))\n",
    "\n",
    "## vectorized softmax in numpy\n",
    "tic = time.time()\n",
    "loss_vec, grad_vec = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized numpy loss: {}, takes {} seconds.'.format(loss_vec, toc - tic))\n",
    "\n",
    "# true value computed by tf\n",
    "W_tf = tf.Variable(W, dtype = tf.float64)\n",
    "tic = time.time()\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(W_tf)\n",
    "    loss_true = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        tf.one_hot(tf.Variable(y_dev, dtype=tf.int32), num_classes), \n",
    "        tf.matmul(tf.Variable(X_dev, dtype=tf.float64), W_tf)\n",
    "    )) + reg * tf.nn.l2_loss(W_tf)\n",
    "    grad_true = tape.gradient(loss_true, W_tf)\n",
    "toc = time.time()\n",
    "print('true loss: {}, takes {} seconds'.format(loss_true, toc - tic))\n",
    "\n",
    "## check the correctness\n",
    "print('*'*100)\n",
    "print('Relative loss error of naive softmax is {}'.format(np.linalg.norm(loss_true - loss_naive)))\n",
    "print('Relative loss error of vectorized softmax is {}'.format(np.linalg.norm(loss_true - loss_vec)))\n",
    "print('Gradient error of naive softmax is {}'.format(np.linalg.norm(grad_true - grad_naive)))\n",
    "print('Gradient error of vectorized softmax is {}'.format(np.linalg.norm(grad_true - grad_vec)))\n",
    "print('*'*100)\n",
    "print('Is naive softmax loss correct? {}'.format(np.allclose(loss_true, loss_naive)))\n",
    "print('Is vectorized softmax loss correct? {}'.format(np.allclose(loss_true, loss_vec)))\n",
    "print('Is naive softmax grad correct? {}'.format(np.allclose(grad_true, grad_naive,1e-02)))\n",
    "print('Is vectorized softmax grad correct? {}'.format(np.allclose(grad_true, grad_vec,1e-02)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Why is there a time difference between the naive method and the vectorized method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Your answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train your classifiers (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1e7981-XHK_f"
   },
   "source": [
    "Now you can start to train your classifiers. Due to the fact that both the logistic regression and softmax classifer does not admit an analytical solution, we are going to use gradient descent algorithm for training. \n",
    "\n",
    "In the training section, you are asked to implement gradient descent optimization method, which can be interpreted as an attempt to minimize the loss function following iterative update of the model parameters using \n",
    "\n",
    "$$w \\gets w - \\alpha \\nabla_w L$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, and $\\nabla_w L$ is the gradient of the loss $L$ w.r.t the parameter $w$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: The code template is given in `./utils/classifiers/basic_classifier.py`. You need to complete functions `train` and `predict`, in the class `BasicClassifier`. Later you will need to use its subclasses `LogisticRegression` and `Softmax` to train the two models seperately and verify your result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H35ZjRpZHK_g"
   },
   "source": [
    "### Train Logistic Regression + Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete the code of subclasses `LogisticRegression` in **./utils/classifiers/basic_classifiers.py**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11111,
     "status": "ok",
     "timestamp": 1559884688501,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "DjRy76pdHK_h",
    "outputId": "ca2707c6-3adf-4bc8-bbce-d30e2a0f0e59",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.classifiers.basic_classifiers import LogisticRegression\n",
    "\n",
    "## The bias trick\n",
    "# We concatenate to every data point an extra dimension of 1\n",
    "X_train_binary_b = np.hstack([X_train_binary, np.ones((X_train_binary.shape[0], 1))])\n",
    "X_val_binary_b = np.hstack([X_val_binary, np.ones((X_val_binary.shape[0], 1))])\n",
    "X_dev_binary_b = np.hstack([X_dev_binary, np.ones((X_dev_binary.shape[0], 1))])\n",
    "\n",
    "## Logistic Regression + SGD\n",
    "classifier = LogisticRegression()\n",
    "reg = 1e-3 # regularization\n",
    "lr = 1e-5 # learning rate\n",
    "\n",
    "loss_hist_sgd = classifier.train(\n",
    "    X_train_binary_b, y_train_binary, \n",
    "    learning_rate=lr, reg=reg, \n",
    "    num_iters=3000, optim='SGD', \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Write the BasicClassifier.predict function and evaluate the performance \n",
    "# on both training set and validation set\n",
    "y_train_pred = classifier.predict(X_train_binary_b)\n",
    "print('training accuracy:', np.mean(y_train_binary == y_train_pred))\n",
    "y_val_pred = classifier.predict(X_val_binary_b)\n",
    "print('validation accuracy:', np.mean(y_val_binary == y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: You may find that the value of loss becomes incorrect here, this is because when creating the binary dataset, we do not map the labels to the labels that a binary classifier want (0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Please modify the code for creating the binary dataset to ensure that the calculated loss during training is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1559884692553,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "E_QGDSgvHK_k",
    "outputId": "70dd1a9c-3603-4faf-d8d2-625fbf050c20"
   },
   "outputs": [],
   "source": [
    "## SGD error plot\n",
    "plt.plot(loss_hist_sgd, label='SGD')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HImk5vMHK_n"
   },
   "source": [
    "### Train Softmax + SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete the code of subclasses **Softmax** in **./utils/classifiers/basic_classifier.py**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11222,
     "status": "ok",
     "timestamp": 1559884706216,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "CR4ZDYF6HK_o",
    "outputId": "02212126-294d-4e77-c7d6-b23b94593ab2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.classifiers.basic_classifiers import Softmax\n",
    "\n",
    "## The bias trick\n",
    "# We concatenate to every data point an extra dimension of 1\n",
    "X_train_b = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val_b = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test_b = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev_b = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "## Softmax + SGD\n",
    "classifier = Softmax()\n",
    "reg = 1e-3 # regularization\n",
    "lr = 1e-5 # learning rate\n",
    "loss_hist_sgd = classifier.train(\n",
    "    X_train_b, y_train, \n",
    "    learning_rate=lr, reg=reg, \n",
    "    num_iters=3000, optim='SGD', \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Write the BasicClassifier.predict function and evaluate the performance \n",
    "# on both the training and validation set\n",
    "y_train_pred = classifier.predict(X_train_b)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = classifier.predict(X_val_b)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 574,
     "status": "ok",
     "timestamp": 1559884709159,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "SCl-qjtlHK_r",
    "outputId": "0d82c109-b1c9-43ea-90c5-6bc6084751af",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## SGD loss curve\n",
    "plt.plot(loss_hist_sgd, label='SGD')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task1-basic_classifiers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "36142657f443a869bd2c1b509e6f1df9b014ad48aa206cdd00d27f8f22cb37ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
